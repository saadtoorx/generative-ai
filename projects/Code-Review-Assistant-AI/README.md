# Code Review Assistant AI

A production-ready code review application powered by a local CodeLlama model via Ollama. Features a FastAPI backend and a modern Streamlit frontend.

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

## âœ¨ Features

- ğŸ” **Multiple Review Types** - Bug Detection, Code Quality, Performance, Security, and General reviews
- ğŸ“Š **Code Statistics** - Automatic calculation of lines, characters, and words
- ğŸ“‹ **Session History** - Track and view your past reviews within the session
- ğŸ“¥ **Export Results** - Download reviews as Markdown files
- âš™ï¸ **Configurable** - Adjust timeouts and model settings
- ğŸ” **Health Status** - Real-time API and Ollama connection monitoring
- ğŸŸ¢ **Visual Feedback** - Color-coded badges and status indicators
- ğŸ”’ **Privacy** - All code processing happens locally on your machine

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     HTTP      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     HTTP      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚   POST/GET    â”‚                 â”‚   /api        â”‚                 â”‚
â”‚   Streamlit     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚    FastAPI      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚     Ollama      â”‚
â”‚   Frontend      â”‚               â”‚    Backend      â”‚               â”‚   (CodeLlama)   â”‚
â”‚   (Port 8501)   â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   (Port 8000)   â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   (Port 11434)  â”‚
â”‚                 â”‚     JSON      â”‚                 â”‚     JSON      â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Prerequisites

- **Python 3.10+** - [Download](https://www.python.org/downloads/)
- **Ollama** - [Install Ollama](https://ollama.ai/download)
- **CodeLlama Model** - Run `ollama pull codellama` after installing Ollama

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/saadtoorx/Code-Review-Assistant-AI
cd "Project 4 Code Review Assistant AI Project"
```

### 2. Create Virtual Environment

```bash
python -m venv venv

# Windows
.\venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure Environment (Optional)

```bash
cp .env.example .env
# Edit .env with your settings
```

### 5. Start Ollama

Make sure Ollama is running:

```bash
ollama serve
```

### 6. Run the Application

**Terminal 1 - Start Backend:**

```bash
python -m uvicorn backend.main:app --reload
```

**Terminal 2 - Start Frontend:**

```bash
streamlit run frontend/app.py
```

### 7. Open the App

Navigate to `http://localhost:8501` in your browser.

## ğŸ“š API Documentation

Once the backend is running, visit:

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Endpoints

| Method | Endpoint   | Description                   |
| ------ | ---------- | ----------------------------- |
| GET    | `/health`  | Check API and Ollama status   |
| POST   | `/review`  | Submit code for review (JSON) |
| POST   | `/review/` | Legacy endpoint (Form data)   |

## ğŸ¨ Screenshots

_Coming soon_

## ğŸ› ï¸ Configuration

Environment variables (set in `.env`):

| Variable          | Default                               | Description                |
| ----------------- | ------------------------------------- | -------------------------- |
| `OLLAMA_URL`      | `http://localhost:11434/api/generate` | Ollama API URL             |
| `MODEL_NAME`      | `codellama`                           | Model to use               |
| `REQUEST_TIMEOUT` | `300`                                 | Request timeout in seconds |
| `BACKEND_URL`     | `http://localhost:8000`               | FastAPI Backend URL        |

## ğŸ“ Project Structure

```
Code Review Assistant AI Project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ llm_service.py  # Ollama integration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py           # Configuration management
â”‚   â”œâ”€â”€ main.py             # FastAPI application
â”‚   â””â”€â”€ models.py           # Pydantic models
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py              # Streamlit application
â”œâ”€â”€ V1/                     # Version 1 of project
â”œâ”€â”€ .env                    # Environment variables
â”œâ”€â”€ .env.example            # Environment template
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai/) - Local LLM runner
- [CodeLlama](https://github.com/facebookresearch/codellama) - State-of-the-art code model
- [FastAPI](https://fastapi.tiangolo.com/) - Modern Python web framework
- [Streamlit](https://streamlit.io/) - Data app framework
